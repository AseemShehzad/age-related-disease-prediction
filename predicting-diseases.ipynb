{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-12T16:25:24.803715Z","iopub.status.busy":"2023-06-12T16:25:24.803337Z","iopub.status.idle":"2023-06-12T16:25:26.079206Z","shell.execute_reply":"2023-06-12T16:25:26.078118Z","shell.execute_reply.started":"2023-06-12T16:25:24.803663Z"},"trusted":true},"source":["### Import the Libraries, Data and Have a Look at the Data!"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n","import tensorflow as tf\n","from tensorflow.keras import regularizers\n","from pandas_profiling import ProfileReport\n","import warnings"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T16:59:48.582272Z","iopub.status.busy":"2023-06-12T16:59:48.581874Z","iopub.status.idle":"2023-06-12T16:59:48.730226Z","shell.execute_reply":"2023-06-12T16:59:48.729024Z","shell.execute_reply.started":"2023-06-12T16:59:48.582241Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(r\"data\\train.csv\")\n","test_df = pd.read_csv(r\"data\\test.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data Cleaning"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The following columns have been dropped: {'BQ', 'EL'} and the remaining missing data is filled using median/mode.\n","The Shape of the train_df is: (617, 56)\n"]}],"source":["def missing_values(df):\n","    \"\"\" This function takes a dataframe as input and returns a dataframe with missing values filled with median/mode and a list of dropped columns. Also, it plots the missing values in a bar chart.\"\"\"\n","\n","    ### Plotting missing values\n","    missing_vals = df.isnull().sum()\n","    missing_vals = missing_vals[missing_vals > 0]\n","    missing_vals.sort_values(inplace=True)\n","    missing_vals.plot.bar()\n","\n","    data_filtered = df.dropna(thresh=len(df) * (1-.05), axis=1).copy()\n","    dropped_columns = set(df.columns.tolist()) - set(data_filtered.columns.tolist())\n","\n","    # Fill missing numeric values with median\n","    numeric_features = data_filtered.select_dtypes(include=[np.number])\n","    for col in numeric_features.columns:\n","        if data_filtered[col].isnull().any():\n","            data_filtered.loc[:, col] = data_filtered[col].fillna(data_filtered[col].median())\n","\n","    # Fill missing categorical values with mode\n","    categorical_features = data_filtered.select_dtypes(exclude=[np.number])\n","    for col in categorical_features.columns:\n","        if data_filtered[col].isnull().any():\n","            data_filtered.loc[:, col] = data_filtered[col].fillna(data_filtered[col].mode()[0])\n","\n","    # Check for missing values and report how many columns have been dropped\n","    print(f\"The following columns have been dropped: {dropped_columns} and the remaining missing data is filled using median/mode.\")\n","\n","    return data_filtered, dropped_columns\n","\n","# Call the function\n","train_df, dropped_columns = missing_values(train_df)\n","print(f\"The Shape of the train_df is: {train_df.shape}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data Exploration"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["### splitting the data into numerical and categorical features and target\n","train_target = train_df['Class']\n","train_cat_features = train_df['EJ']\n","train_num_features = train_df.drop(['Id', 'Class', 'EJ'], axis=1)\n","\n","### Standardizing the numerical features\n","num_cols = train_num_features.columns.tolist()\n","scaler = StandardScaler()\n","train_num_features = scaler.fit_transform(train_num_features)\n","train_num_features = pd.DataFrame(train_num_features, columns=num_cols)\n","\n","### One-hot encoding the categorical features\n","train_cat_features = pd.get_dummies(train_cat_features)\n","\n","### Concatenating the numerical and categorical features\n","train_features = pd.concat([train_num_features, train_cat_features], axis=1)\n","\n","### Concatenating the features and target\n","train_data = pd.concat([train_features, train_target], axis=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Dealing with Outliers"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["31 Outliers Detected\n","Shape of the dataframe after dropping outliers: (586, 56)\n"]}],"source":["### stopping the warnings\n","warnings.filterwarnings('ignore')\n","from sklearn.ensemble import IsolationForest\n","\n","def run_isolation_forest(df, contamination=0.025, max_features=2):\n","    # Fit the model\n","    isof = IsolationForest(random_state=42, contamination=contamination, max_features=max_features).fit(df)\n","    results = isof.predict(df)\n","    df['isolation scores'] = results\n","    df['is_outlier'] = np.where(df['isolation scores'] == -1, 1, 0)\n","    print(f\"{len(df[df['is_outlier'] == 1])} Outliers Detected\")\n","    df = df.drop(['isolation scores'], axis=1)\n","    \n","    return df\n","\n","### Running the isolation forest\n","### Specify the contamination and max_features\n","train_data = run_isolation_forest(train_data, contamination=0.05, max_features=5)\n","\n","### Plotting the outliers\n","sns.pairplot(train_data[[\"is_outlier\", \"AB\", 'AH', 'AM', 'CL', 'CR', 'CS']], hue=\"is_outlier\")\n","plt.show()\n","\n","### Dropping the outliers\n","train_data = train_data[train_data['is_outlier'] == 0]\n","train_data = train_data.drop(['is_outlier'], axis=1)\n","print(f\"Shape of the dataframe after dropping outliers: {train_data.shape}\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-06-12T16:31:37.412920Z","iopub.status.busy":"2023-06-12T16:31:37.411791Z","iopub.status.idle":"2023-06-12T16:31:49.196850Z","shell.execute_reply":"2023-06-12T16:31:49.195754Z","shell.execute_reply.started":"2023-06-12T16:31:37.412871Z"},"trusted":true},"outputs":[],"source":["# Get all the columns except 'Class'\n","columns = train_data.columns.tolist()\n","\n","# Calculate the number of rows needed for subplots\n","n = len(columns)\n","ncols = 2\n","nrows = n // ncols + (n % ncols > 0)\n","\n","# Create a figure and axes for subplots\n","fig, axs = plt.subplots(nrows, ncols, figsize=(10, 4*nrows))  # Adjust as per your requirement\n","axs = axs.flatten()  # to make it easier to iterate over\n","\n","# For each column and its corresponding subplot, create a boxplot\n","for ax, col in zip(axs, columns):\n","    sns.boxplot(data=train_data, x='Class', y=col, ax=ax)\n","    ax.set_title(f'Class vs {col}')\n","\n","# If there are less columns than subplots, remove the extra ones\n","for i in range(n, nrows*ncols):\n","    fig.delaxes(axs[i])\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-06-12T13:41:29.769371Z","iopub.status.busy":"2023-06-12T13:41:29.768925Z","iopub.status.idle":"2023-06-12T13:41:29.776713Z","shell.execute_reply":"2023-06-12T13:41:29.774986Z","shell.execute_reply.started":"2023-06-12T13:41:29.769339Z"}},"source":["After data visualization, here are some takeaways:\n","1. There are no strong relations of any variable with the output. So, it would be difficult to correctly classify output, especially when there is so small dataset.\n","2. There are many outliers in the dataset that do no follow the general patterns of the data. So, we are going to cap the data to 0.05 and 0.95 percentile."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data Pre-processing"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["train_features = train_data.drop(['Class'], axis=1)\n","train_target = train_data['Class']"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["### Winsorizing the data\n","import pandas as pd\n","\n","def winsorize_df(df):\n","    for col in df.columns:\n","        if df[col].dtype.kind in 'biufc':\n","            lower = df[col].quantile(0.02)\n","            upper = df[col].quantile(0.98)\n","            df[col] = df[col].clip(lower, upper)\n","    return df\n","\n","train_features_capped = train_features.copy()\n","train_features_capped = winsorize_df(train_features_capped)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data Modeling"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Split the data into train and test sets\n","y = train_data['Class']\n","X = train_features\n","\n","# Split the data into train and validation sets\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=22)\n","\n","# Convert the data to TensorFlow tensors\n","X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n","X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Random Forest\n","\n","After applying Gradient Boosing Classifier, Random Forest Classifier and Ada Boost Classifier, it was found that Random Forest outperformed other trees and thus, only RFs predictions are shown here."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.89      0.97      0.93       101\n","           1       0.62      0.29      0.40        17\n","\n","    accuracy                           0.87       118\n","   macro avg       0.76      0.63      0.66       118\n","weighted avg       0.85      0.87      0.85       118\n","\n"]}],"source":["# Applying LGBM Classifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","# Create a Random Forest Classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=22, max_depth=5)\n","\n","# Train the model using the training sets\n","rf.fit(X_train, y_train)\n","\n","# Predict the response for test dataset\n","y_pred = rf.predict(X_val)\n","\n","# Plotting the probability distribution of the predictions\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1)\n","sns.distplot(y_pred, bins=20, kde=False)\n","plt.title('Probability Distribution of Predictions', size=6)\n","plt.show()\n","\n","### Plotting a confusion matrix\n","cm = confusion_matrix(y_val, y_pred)\n","sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n","plt.xlabel('Predicted')\n","plt.ylabel('Truth')\n","plt.show()\n","\n","### Getting the precision and recall scores\n","print(classification_report(y_val, y_pred))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature</th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31</th>\n","      <td>DU</td>\n","      <td>0.109032</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>FL</td>\n","      <td>0.090272</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>GL</td>\n","      <td>0.076823</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>CR</td>\n","      <td>0.069184</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>EH</td>\n","      <td>0.047829</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   feature  importance\n","31      DU    0.109032\n","44      FL    0.090272\n","52      GL    0.076823\n","20      CR    0.069184\n","37      EH    0.047829"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["### Printing the feature importance of 10 most important features\n","feature_importances = rf.feature_importances_\n","feature_importances = pd.DataFrame({'feature': list(train_features.columns), 'importance': feature_importances}).sort_values('importance', ascending=False)\n","feature_importances.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### MLP Neural Network"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["### Creating a MLP neural network model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow import keras\n","\n","\n","# Create a Sequential model\n","model = Sequential()\n","\n","# Add regularization (l2) to the layers\n","model.add(Dense(100, activation='relu', input_dim=X_train.shape[1], kernel_regularizer=regularizers.l2(0.015)))\n","model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.05)))\n","model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.05)))\n","model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.05)))\n","model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","# Use sigmoid activation for binary classification\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Add gradient clipping to Adam optimizer\n","adam = Adam(learning_rate=.0001, clipvalue=0.5)\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","\n","# Add early stopping to prevent overfitting\n","early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=300, batch_size=5, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, class_weight={0:1, 1:1})"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Plot training & validation accuracy values\n","plt.figure(figsize=(10, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","\n","# Plot training & validation loss values\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Predicting the test set results\n","y_pred = model.predict(X_val)\n","\n","# Plotting the probability distribution of the predictions\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1)\n","sns.distplot(y_pred, bins=20, kde=False)\n","plt.title('Probability Distribution of Predictions', size=6)\n","\n","y_pred = (y_pred > 0.5)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.99      0.97       101\n","           1       0.92      0.71      0.80        17\n","\n","    accuracy                           0.95       118\n","   macro avg       0.94      0.85      0.89       118\n","weighted avg       0.95      0.95      0.95       118\n","\n"]}],"source":["### Plotting a confusion matrix\n","cm = confusion_matrix(y_val, y_pred)\n","sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n","plt.xlabel('Predicted')\n","plt.ylabel('Truth')\n","plt.show()\n","\n","### Getting the precision and recall scores\n","print(classification_report(y_val, y_pred))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["### Prepare the test data\n","test_num_features = test_df.drop(['Id', 'EJ', 'BQ', 'EL'], axis=1)\n","test_cat_features = test_df['EJ']\n","\n","# Scale the numerical features\n","num_cols = test_num_features.columns\n","test_num_features = scaler.transform(test_num_features)\n","test_num_features = pd.DataFrame(test_num_features, columns=num_cols)\n","\n","# Convert the categorical features to one-hot encoded features\n","test_cat_features = pd.get_dummies(test_cat_features)\n","test_cat_features['B'] = 0\n","\n","# Concatenate the numerical and categorical features\n","test_features = np.concatenate((test_num_features, test_cat_features), axis=1)\n","\n","# Convert the data to TensorFlow tensors\n","test_features = tf.convert_to_tensor(test_features, dtype=tf.float32)\n","\n","# Predict the test set results\n","y_pred = model.predict(test_features)\n","\n","# Plotting the probability distribution of the predictions\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1)\n","sns.distplot(y_pred, bins=20, kde=False)\n","plt.title('Probability Distribution of Predictions', size=6)\n","plt.show()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["### Creating a submission file with column names 'Id', 'Class0', 'Class1'\n","submission_df = pd.DataFrame(columns=['Id', 'class_0', 'class_1'])\n","submission_df['Id'] = test_df['Id']\n","submission_df['class_0'] = 1 - y_pred\n","submission_df['class_1'] = y_pred\n","\n","# Save the submission file\n","submission_df.to_csv('submission.csv', index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Results\n","\n","1. It was surprising to see Neural Network outperform the decision trees in this problem.\n","2. A precision of 97% was achieved with the current neural network and data."]}],"metadata":{"kernelspec":{"display_name":"TensorFlow Env","language":"python","name":"dp2env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
